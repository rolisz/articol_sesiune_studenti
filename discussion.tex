\section{Discussion and Comparison to Related Work}
\label{sec:disc}

This section analyzes our approaches and compares them to similar existing work.

\subsection{Analysis of the Proposed Approach}
As shown in section \ref{sec:recog}, using an SVM with a RBF kernel seems to lead to slightly better results. One explanation for this would be that the data is already high dimensional (400 dimensions) and the data points lie in sufficiently different parts so that the separating hyperplanes work effectively, without needing a projection into the infinite dimensional space offered by the RBF kernel. 

In the case of the character segmentation, the values of the parameter didn't matter that much. The difference between the best and the worst result was about 0.2. The number of features had the most influence, of $ 0.1\% $, on the F1 score, while the number of trees influenced it only by 0.01. The highest average was obtained for the values of 250 trees in the forest and 40 features chosen at each split, giving an F1 score of $ 87.936\% \pm 0.445$ on the validation set and a score of $ 91.99\% $ on the test set.  

This seems to indicate that the number of features that are chosen at each split point is a more important parameter for random forests, while the number of trees used in the forest doesn't vary the results so much. This is in line with the theoretical basis of random forests\cite{breiman2001random}, which suggest that random forests are somewhat sensitive to the number of features used in them. 

If we look at the confusion matrix for the character recognition problem, computed for the whole data set, we can see that one of the most often made mistakes are:
\begin{itemize}
\item confusing , with . - 28 times
\item confusing . with , - 12 times
\item confusing O with 0 - 24 times
\item confusing 0 with O - 15 times
\item confusing 0 with o - 3 times
\item confusing l with I - 6 times
\item confusing I with T - 5 times
\item confusing I with 1 - 4 times
\item confusing 1 with I - 4 times
\end{itemize} 

These mistakes are easy to make: a comma and a period are very similar, especially in a noisy, low resolution environment. The lower and upper case letters O are again very similar to the digit 0, on some receipts there being no distinction between O and 0, only the context in which they are used. For monospace fonts, which some receipts use, the lack of difference between l, I, 1 is a well known problem, which is why many design books and human computer interaction studies recommend using other fonts\cite{chaparro2006examining}. 

If we look at the confusion matrix for the character segmentation problem, we see that it makes the mistake of not predicting a split more often than it does the opposite, so the model is more specific, rather then sensitive. 

Specifity and sensitivity are defined as\cite{Fawcett_2006}

\[
    \text{sensitivity} = \frac{\text{true positives}}{\text{all positives}} = \frac{2546}{2546+363} = 87.52 \%
\]
\[
    \text{specificity} = \frac{\text{true negatives}}{\text{all negatives}} = \frac{4556}{4556+255} =  94.70 \%
\]

\subsection{Comparison to Related Work}
If we compare the results of the character recognition problem with the ones obtained on the MNIST dataset\cite{lecun1998mnist}, we see that our error rates are much higher: around 9\% versus 1.4\% (obtained by using a Gaussian SVM on MNIST). There are two possible explanations for this. One is that MNIST has only ten classes (corresponding to the ten digits), so it is much easier for a classifier to assign the correct class to a data point, compared to our dataset, which has 74 distinct classes, of which many are similar (the digit 0 and the letters o and O, the digit 1 and the letter l). Another factor that makes MNIST easier to solve is the fact that it has 70000 datapoints in total, while our dataset contains only 7045 data points. This almost ten fold difference has a great contribution to the ability of the classifier to generalize. Even on the MNIST dataset, adding more data is beneficial, as show by Ciresan et al.\cite{Cire_an_2010}, who obtained one of the best results on MNIST by augmenting the dataset with artificial data generated by transforming the existing digits. 

The results obtained in \cite{kahraman2003license} and \cite{Franc_2005} for the character segmentation problem are a bit better than our results: 94.5\% and 96.7\% accuracy, compared to our 91.9\%. The difference between the two problems is that in their papers the number of characters in a licence plate is known and fixed, while lines in a receipt have varying lenghts. This prior knowledge used by them can explain the difference in results. 

The paper by Janssen\cite{janssen2012receipts2go} deals with OCR on receipts, but they don't train the OCR engine for receipts, they just present a new approach for image normalization.

\subsection{Application for the OCR Engine}
The main reason for developing this OCR engine was to use it an application, called ReceiptBudget, which would enable users to take a photo of a receipt, the program would then extract the relevant information (date, items, store, total) from the photo, store it in a database, and then user could view an interactive dashboard of his expenses.

The motivation for using this method of data entry is that it is much faster than the alternative, which is much slower, tedious and users are prone to forgetting to do it. 

One view of the dashboard would allow the user to view a heat map of his expenses, either as a total or on a timeline. Another view would allow the user to filter expenses based on shop, month, day of week and date range, in order to notice patterns in his spending. 

The application was presented at the Imprezzio Software Contest in November 2013, where it won the first prize\cite{WinNT}.
