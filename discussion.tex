\section{Discussion and comparison to related work}

This section analyzes our approaches and compares them to similar existing work.

\subsection{Analysis of the proposed approach}
For the character recognition problem, using a RBF kernel SVM resulted in a $ 91.018\% \pm 0.126 $ error rate in the best case, using a value of 100 for the regularization rate, while using a linear kernel yielded $ 90.490\% \pm 0.097 $ as its best result, for the value of 1 for the regularization rate. Using a RBF kernel gives a slight improvement in performance.

One explanation for this would be that the data is already high dimensional (400 dimensions) and the data points lie in sufficiently different parts so that the separating hyperplanes work effectively, without needing a projection into the infinite dimensional space offered by the RBF kernel. 

In the case of the character segmentation, the values of the parameter didn't matter that much. The difference between the best and the worst result was about 0.2. The number of features had the most influence, of 0.1\%, on the F1 score, while the number of trees influenced it only by 0.01. The highest average was obtained for the values of 250 trees in the forest and 40 features chosen at each split, giving an F1 score of $ 87.936\% \pm 0.445$ on the validation set and a score of $ 91.99\% $ on the test set.  

This seems to indicate that the number of features that are chosen at each split point is a more important parameter for random forests, while the number of trees used in the forest doesn't vary the results so much. This is in line with the theoretical basis of random forests\cite{breiman2001random}, which suggest that random forests are somewhat sensitive to the number of features used in them. 

If we look at the confusion matrix for the character recognition problem, computed for the whole data set, we can see that one of the most often made mistakes are:
\begin{itemize}
\item confusing , with . - 28 times
\item confusing . with , - 12 times
\item confusing O with 0 - 24 times
\item confusing 0 with O - 15 times
\item confusing 0 with o - 3 times
\item confusing l with I - 6 times
\item confusing I with T - 5 times
\item confusing I with 1 - 4 times
\item confusing 1 with I - 4 times
\end{itemize} 

These mistakes are easy to make: a comma and a period are very similar, especially in a noisy, low resolution environment. The lower and upper case letters O are again very similar to the digit 0, on some receipts there being no distinction between O and 0, only the context in which they are used. For monospace fonts, which some receipts use, the lack of difference between l, I, 1 is a well known problem, which is why many design books recommend using other fonts. 

If we look at the confusion matrix for the character segmentation problem, we see that it makes the mistake of not predicting a split more often than it does the opposite, so the model is more specific, rather then sensitive. 

Specifity and sensitivity are defined as\cite{Fawcett_2006}

\[
    \text{sensitivity} = \frac{positives correctly classified}{total positives} = \frac{2546}{2546+363} = 87.52 \%
    
    \text{specificity} = \frac{true negatives}{true negatives + false positives} = \frac{4556}{4556+255} =  94.70 \%
\]

\subsection{Comparison to related work}
If we compare the results of the character recognition problem with the ones obtained on the MNIST dataset\cite{lecun1998mnist}, we see that our error rates are much higher: around 9\% versus 1.4\% (obtained by using a Gaussian SVM on MNIST). There are two possible explanations for this. One is that MNIST has only ten classes (corresponding to the ten digits), so it is much easier for a classifier to assign the correct class to a data point, compared to our dataset, which has 74 distinct classes, of which many are similar (the digit 0 and the letters o and O, the digit 1 and the letter l). Another factor that makes MNIST easier to solve is the fact that it has 70000 datapoints in total, while our dataset contains only 7045 data points. This almost ten fold difference has a great contribution to the ability of the classifier to generalize. Even on the MNIST dataset, adding more data is beneficial, as show by Ciresan et al.\cite{Cire_an_2010}, who obtained one of the best results on MNIST by augmenting the dataset with artificial data generated by transforming the existing digits. 

The results obtained in \cite{kahraman2003license} and \cite{Franc_2005} for the character segmentation problem are a bit better than our results: 94.5\% and 96.7\% accuracy, compared to our 91.9\%. The difference between the two problems is that in their papers the number of characters in a licence plate is known and fixed, while lines in a receipt have varying lenghts. This prior knowledge used by them can explain the difference in results. 

The paper by Janssen\cite{janssen2012receipts2go} deals with OCR on receipts, but they don't train the OCR engine for receipts, they just present a new approach for image normalization.

\subsection{Application for the OCR engine}


si ceva grafice 

aici vine si despre aplicatie


